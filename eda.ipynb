{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fc45dcc",
   "metadata": {},
   "source": [
    "# Next Up: Exploratory Data Analysis\n",
    "\n",
    "## Project Overview\n",
    "**Goal**: Predict which G-League players will be called up to the NBA based on performance metrics, demographics, and contextual factors.\n",
    "\n",
    "**Target Variable**: `called_up` (binary: 1 = called up, 0 = not called up)\n",
    "\n",
    "**Dataset**: `data/prediction_dataset.csv` - Combined dataset with player stats and callup information\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "1. **Data Quality**: Assess missing values, data types, and data quality\n",
    "2. **Target Analysis**: Understand class distribution and call-up patterns\n",
    "3. **Feature Analysis**: Identify features most correlated with call-ups\n",
    "4. **Data Cleaning**: Handle missing values, outliers, and prepare data for modeling\n",
    "5. **Feature Selection**: Identify top predictive features for modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f8c3e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Setup and Data Loading\n",
    "\n",
    "Import libraries and load the prediction dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112381f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54deff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prediction dataset\n",
    "df = pd.read_csv('data/prediction_dataset.csv')\n",
    "\n",
    "print(f\"✅ Dataset loaded successfully\")\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"  - Rows: {df.shape[0]:,}\")\n",
    "print(f\"  - Columns: {df.shape[1]}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca2adad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Data Quality Assessment\n",
    "\n",
    "Assess data quality, missing values, and data types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f616164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"  - Total records: {len(df):,}\")\n",
    "print(f\"  - Total columns: {len(df.columns)}\")\n",
    "print(f\"\\nData Types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12091b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Missing %': missing_pct\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "print(\"Missing Values Analysis:\")\n",
    "print(f\"  - Columns with missing values: {len(missing_df)}\")\n",
    "print(f\"  - Total missing values: {missing.sum():,}\")\n",
    "print(f\"\\nTop columns with missing values:\")\n",
    "print(missing_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871425d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Duplicate rows: {duplicates}\")\n",
    "\n",
    "# Check for duplicate player-season combinations\n",
    "if 'player_name' in df.columns and 'season_year' in df.columns:\n",
    "    player_season_dups = df.duplicated(subset=['player_name', 'season_year']).sum()\n",
    "    print(f\"Duplicate player-season combinations: {player_season_dups}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bcfc45",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Target Variable Analysis\n",
    "\n",
    "Analyze the distribution of the target variable `called_up` and understand class imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5ceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "target_counts = df['called_up'].value_counts()\n",
    "target_pct = df['called_up'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Target Variable Distribution:\")\n",
    "print(f\"  - Called Up (1): {target_counts[1]:,} ({target_pct[1]:.2f}%)\")\n",
    "print(f\"  - Not Called Up (0): {target_counts[0]:,} ({target_pct[0]:.2f}%)\")\n",
    "print(f\"  - Total: {len(df):,}\")\n",
    "\n",
    "# Visualize target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "axes[0].bar(['Not Called Up (0)', 'Called Up (1)'], [target_counts[0], target_counts[1]], \n",
    "            color=['#3498db', '#e74c3c'])\n",
    "axes[0].set_title('Target Variable Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "for i, v in enumerate([target_counts[0], target_counts[1]]):\n",
    "    axes[0].text(i, v + 20, f'{v:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Percentage plot\n",
    "axes[1].bar(['Not Called Up (0)', 'Called Up (1)'], [target_pct[0], target_pct[1]], \n",
    "            color=['#3498db', '#e74c3c'])\n",
    "axes[1].set_title('Target Variable Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Percentage (%)')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "for i, v in enumerate([target_pct[0], target_pct[1]]):\n",
    "    axes[1].text(i, v + 1, f'{v:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n⚠️ Class Imbalance: {target_pct[0]:.1f}% vs {target_pct[1]:.1f}%\")\n",
    "print(\"   This is an imbalanced classification problem.\")\n",
    "print(\"   We'll need to use appropriate metrics (F1, precision/recall) and techniques (class weights, SMOTE).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08848721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call-up rate by season\n",
    "if 'season_year' in df.columns:\n",
    "    season_analysis = df.groupby('season_year').agg({\n",
    "        'called_up': ['sum', 'count', 'mean']\n",
    "    }).round(3)\n",
    "    season_analysis.columns = ['Called_Up', 'Total_Players', 'Call_Up_Rate']\n",
    "    season_analysis['Call_Up_Rate_Pct'] = (season_analysis['Call_Up_Rate'] * 100).round(2)\n",
    "    \n",
    "    print(\"Call-up Rate by Season:\")\n",
    "    print(season_analysis)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.bar(season_analysis.index.astype(str), season_analysis['Call_Up_Rate_Pct'], \n",
    "           color='#e74c3c', alpha=0.7)\n",
    "    ax.set_title('Call-up Rate by Season', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Season Year')\n",
    "    ax.set_ylabel('Call-up Rate (%)')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    for i, v in enumerate(season_analysis['Call_Up_Rate_Pct']):\n",
    "        ax.text(i, v + 0.5, f'{v:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99349dba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3.5: Exclude Pandemic Year (2021)\n",
    "\n",
    "Exclude the 2021 season from analysis as it was during the COVID-19 pandemic and may have different patterns that don't reflect normal call-up behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2367e321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out 2021 season (pandemic year)\n",
    "print(\"Filtering out 2021 season (pandemic year)...\")\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# Check current season distribution\n",
    "if 'season_year' in df.columns:\n",
    "    print(f\"\\nSeason distribution before filtering:\")\n",
    "    season_counts = df['season_year'].value_counts().sort_index()\n",
    "    print(season_counts)\n",
    "    \n",
    "    # Filter out 2021\n",
    "    df = df[df['season_year'] != 2021].copy()\n",
    "    \n",
    "    print(f\"\\nAfter filtering out 2021:\")\n",
    "    print(f\"  - Removed: {len(df[df['season_year'] == 2021]) if 2021 in df['season_year'].values else 0} records\")\n",
    "    print(f\"  - Remaining records: {len(df):,}\")\n",
    "    print(f\"  - New shape: {df.shape}\")\n",
    "    \n",
    "    print(f\"\\nSeason distribution after filtering:\")\n",
    "    season_counts_after = df['season_year'].value_counts().sort_index()\n",
    "    print(season_counts_after)\n",
    "    \n",
    "    # Update target variable statistics\n",
    "    target_counts = df['called_up'].value_counts()\n",
    "    target_pct = df['called_up'].value_counts(normalize=True) * 100\n",
    "    print(f\"\\nUpdated target distribution:\")\n",
    "    print(f\"  - Called Up (1): {target_counts[1]:,} ({target_pct[1]:.2f}%)\")\n",
    "    print(f\"  - Not Called Up (0): {target_counts[0]:,} ({target_pct[0]:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\n✅ 2021 season excluded from analysis\")\n",
    "else:\n",
    "    print(\"⚠️ Warning: 'season_year' column not found. Cannot filter 2021.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a16f373",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Data Cleaning and Preparation\n",
    "\n",
    "Clean the dataset by handling missing values, removing unnecessary columns, and preparing features for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1031a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for cleaning\n",
    "df_clean = df.copy()\n",
    "\n",
    "print(\"Starting data cleaning...\")\n",
    "print(f\"Original shape: {df_clean.shape}\")\n",
    "\n",
    "# Identify columns to drop (ID columns, redundant columns, callup details for non-called-up players)\n",
    "cols_to_drop = [\n",
    "    'season_id', 'team_id', 'player_id',  # ID columns\n",
    "    'callup_date', 'callup_nba_team', 'callup_contract_type',  # Only relevant for called_up=1\n",
    "    'season_type',  # All same value (REG)\n",
    "]\n",
    "\n",
    "# Drop columns that exist\n",
    "cols_to_drop = [col for col in cols_to_drop if col in df_clean.columns]\n",
    "df_clean = df_clean.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"After dropping ID/redundant columns: {df_clean.shape}\")\n",
    "print(f\"Dropped columns: {cols_to_drop}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b9ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numeric and categorical columns\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove target variable from numeric cols for feature analysis\n",
    "if 'called_up' in numeric_cols:\n",
    "    numeric_cols.remove('called_up')\n",
    "if 'season_year' in numeric_cols:\n",
    "    numeric_cols.remove('season_year')  # Keep as is for now\n",
    "\n",
    "print(f\"Numeric columns: {len(numeric_cols)}\")\n",
    "print(f\"Categorical columns: {len(categorical_cols)}\")\n",
    "print(f\"\\nCategorical columns: {categorical_cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d14200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values in numeric columns\n",
    "# Strategy: For stats columns, missing values likely mean 0 (player didn't play)\n",
    "# For percentages, we'll impute with median\n",
    "\n",
    "print(\"Handling missing values in numeric columns...\")\n",
    "\n",
    "# Identify columns with missing values\n",
    "missing_numeric = df_clean[numeric_cols].isnull().sum()\n",
    "missing_numeric = missing_numeric[missing_numeric > 0]\n",
    "\n",
    "print(f\"Numeric columns with missing values: {len(missing_numeric)}\")\n",
    "if len(missing_numeric) > 0:\n",
    "    print(missing_numeric)\n",
    "\n",
    "# For total/avg stats: impute with 0 (player didn't play/record stat)\n",
    "# For percentages: impute with median\n",
    "total_avg_cols = [col for col in numeric_cols if any(x in col.lower() for x in ['total_', 'avg_', 'games', 'minutes', 'points', 'rebounds', 'assists', 'steals', 'blocks', 'turnovers', 'fgm', 'fga', 'tpm', 'tpa', 'ftm', 'fta'])]\n",
    "pct_cols = [col for col in numeric_cols if 'pct' in col.lower() or '_pct' in col.lower()]\n",
    "\n",
    "# Impute total/avg stats with 0\n",
    "for col in total_avg_cols:\n",
    "    if col in df_clean.columns and df_clean[col].isnull().sum() > 0:\n",
    "        df_clean[col] = df_clean[col].fillna(0)\n",
    "\n",
    "# Impute percentages with median\n",
    "for col in pct_cols:\n",
    "    if col in df_clean.columns and df_clean[col].isnull().sum() > 0:\n",
    "        median_val = df_clean[col].median()\n",
    "        df_clean[col] = df_clean[col].fillna(median_val)\n",
    "\n",
    "# For any remaining numeric columns, fill with median\n",
    "remaining_numeric = [col for col in numeric_cols if df_clean[col].isnull().sum() > 0]\n",
    "for col in remaining_numeric:\n",
    "    median_val = df_clean[col].median()\n",
    "    df_clean[col] = df_clean[col].fillna(median_val)\n",
    "\n",
    "print(f\"✅ Missing values handled\")\n",
    "print(f\"Remaining missing values: {df_clean[numeric_cols].isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b6c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle infinite values (replace with NaN then impute)\n",
    "inf_cols = []\n",
    "for col in numeric_cols:\n",
    "    if np.isinf(df_clean[col]).any():\n",
    "        inf_cols.append(col)\n",
    "        df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "        df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "\n",
    "if len(inf_cols) > 0:\n",
    "    print(f\"Fixed infinite values in: {inf_cols}\")\n",
    "else:\n",
    "    print(\"✅ No infinite values found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe2ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with zero variance (constant values)\n",
    "zero_var_cols = []\n",
    "for col in numeric_cols:\n",
    "    if df_clean[col].nunique() <= 1:\n",
    "        zero_var_cols.append(col)\n",
    "\n",
    "if len(zero_var_cols) > 0:\n",
    "    print(f\"Removing {len(zero_var_cols)} columns with zero variance: {zero_var_cols}\")\n",
    "    df_clean = df_clean.drop(columns=zero_var_cols)\n",
    "    numeric_cols = [col for col in numeric_cols if col not in zero_var_cols]\n",
    "\n",
    "print(f\"Final shape after cleaning: {df_clean.shape}\")\n",
    "print(f\"Remaining numeric features: {len(numeric_cols)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ea3258",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Feature Correlation Analysis\n",
    "\n",
    "Analyze correlations between features and the target variable to identify the most predictive features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a158dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with target variable\n",
    "correlations = df_clean[numeric_cols + ['called_up']].corr()['called_up'].sort_values(ascending=False)\n",
    "correlations = correlations.drop('called_up')  # Remove self-correlation\n",
    "\n",
    "print(\"Top 20 Features Most Correlated with 'called_up':\")\n",
    "print(\"=\" * 60)\n",
    "top_positive = correlations.head(20)\n",
    "for idx, (feature, corr) in enumerate(top_positive.items(), 1):\n",
    "    print(f\"{idx:2d}. {feature:40s} {corr:7.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nBottom 10 Features (Negative Correlation):\")\n",
    "bottom_negative = correlations.tail(10)\n",
    "for idx, (feature, corr) in enumerate(bottom_negative.items(), 1):\n",
    "    print(f\"{idx:2d}. {feature:40s} {corr:7.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa0fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top correlations\n",
    "top_n = 25\n",
    "top_features = correlations.abs().nlargest(top_n).index.tolist()\n",
    "top_corrs = correlations[top_features]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "colors = ['#e74c3c' if x > 0 else '#3498db' for x in top_corrs.values]\n",
    "ax.barh(range(len(top_features)), top_corrs.values, color=colors, alpha=0.7)\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features, fontsize=10)\n",
    "ax.set_xlabel('Correlation with called_up', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'Top {top_n} Features Correlated with Call-ups', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(top_corrs.values):\n",
    "    ax.text(v + 0.01 if v > 0 else v - 0.01, i, f'{v:.3f}', \n",
    "            va='center', ha='left' if v > 0 else 'right', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d00f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation heatmap for top features\n",
    "top_features_for_heatmap = correlations.abs().nlargest(15).index.tolist()\n",
    "corr_matrix = df_clean[top_features_for_heatmap + ['called_up']].corr()\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, vmin=-1, vmax=1)\n",
    "plt.title('Correlation Heatmap: Top 15 Features vs Target', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039f8086",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Statistical Analysis by Target Group\n",
    "\n",
    "Compare feature distributions between called-up and not-called-up players using statistical tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b8f3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison: called_up=1 vs called_up=0\n",
    "called_up = df_clean[df_clean['called_up'] == 1]\n",
    "not_called_up = df_clean[df_clean['called_up'] == 0]\n",
    "\n",
    "print(\"Statistical Comparison: Called Up vs Not Called Up\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select top features for detailed analysis\n",
    "top_features_stats = correlations.abs().nlargest(15).index.tolist()\n",
    "\n",
    "results = []\n",
    "for feature in top_features_stats:\n",
    "    if feature in df_clean.columns:\n",
    "        group1 = called_up[feature].dropna()\n",
    "        group2 = not_called_up[feature].dropna()\n",
    "        \n",
    "        if len(group1) > 0 and len(group2) > 0:\n",
    "            # T-test\n",
    "            t_stat, p_value = stats.ttest_ind(group1, group2)\n",
    "            \n",
    "            results.append({\n",
    "                'Feature': feature,\n",
    "                'Called_Up_Mean': group1.mean(),\n",
    "                'Not_Called_Up_Mean': group2.mean(),\n",
    "                'Difference': group1.mean() - group2.mean(),\n",
    "                'T_Statistic': t_stat,\n",
    "                'P_Value': p_value,\n",
    "                'Significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "            })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('P_Value')\n",
    "\n",
    "print(\"\\nTop Features with Significant Differences (p < 0.05):\")\n",
    "print(results_df[results_df['P_Value'] < 0.05][['Feature', 'Called_Up_Mean', 'Not_Called_Up_Mean', \n",
    "                                                  'Difference', 'P_Value', 'Significant']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e7cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions for top features\n",
    "top_5_features = correlations.abs().nlargest(5).index.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(top_5_features):\n",
    "    if idx < len(axes):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Remove outliers for better visualization (using IQR method)\n",
    "        Q1 = df_clean[feature].quantile(0.25)\n",
    "        Q3 = df_clean[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        data_called = called_up[(called_up[feature] >= lower_bound) & (called_up[feature] <= upper_bound)][feature]\n",
    "        data_not_called = not_called_up[(not_called_up[feature] >= lower_bound) & (not_called_up[feature] <= upper_bound)][feature]\n",
    "        \n",
    "        ax.hist(data_not_called, bins=30, alpha=0.6, label='Not Called Up', color='#3498db', density=True)\n",
    "        ax.hist(data_called, bins=30, alpha=0.6, label='Called Up', color='#e74c3c', density=True)\n",
    "        ax.set_title(f'{feature}\\\\n(Corr: {correlations[feature]:.3f})', fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "# Remove extra subplot\n",
    "if len(axes) > 5:\n",
    "    fig.delaxes(axes[5])\n",
    "\n",
    "plt.suptitle('Distribution Comparison: Top 5 Features', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73849462",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Categorical Feature Analysis\n",
    "\n",
    "Analyze categorical features (position, etc.) and their relationship with call-ups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8334c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze position if available\n",
    "if 'position' in df_clean.columns:\n",
    "    position_analysis = df_clean.groupby('position').agg({\n",
    "        'called_up': ['sum', 'count', 'mean']\n",
    "    }).round(3)\n",
    "    position_analysis.columns = ['Called_Up', 'Total_Players', 'Call_Up_Rate']\n",
    "    position_analysis = position_analysis.sort_values('Call_Up_Rate', ascending=False)\n",
    "    \n",
    "    print(\"Call-up Rate by Position:\")\n",
    "    print(position_analysis)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    positions = position_analysis.index\n",
    "    call_up_rates = position_analysis['Call_Up_Rate'] * 100\n",
    "    \n",
    "    bars = ax.bar(positions, call_up_rates, color='#e74c3c', alpha=0.7)\n",
    "    ax.set_title('Call-up Rate by Position', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Call-up Rate (%)')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for i, (bar, rate) in enumerate(zip(bars, call_up_rates)):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{rate:.1f}% (n={position_analysis.iloc[i][\"Total_Players\"]})',\n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Position column not found in dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39adf6e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Feature Selection Summary\n",
    "\n",
    "Summarize the most important features for modeling based on correlation analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0afe0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top features for modeling\n",
    "# Criteria: Absolute correlation > 0.05 and statistically significant\n",
    "top_features_for_modeling = correlations.abs().nlargest(30).index.tolist()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP FEATURES FOR MODELING\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\\\nSelected {len(top_features_for_modeling)} features based on correlation analysis:\")\n",
    "print(\"\\\\nTop 30 Features (by absolute correlation with target):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for idx, feature in enumerate(top_features_for_modeling, 1):\n",
    "    corr_val = correlations[feature]\n",
    "    print(f\"{idx:2d}. {feature:45s} Correlation: {corr_val:7.4f}\")\n",
    "\n",
    "# Save feature list\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': top_features_for_modeling,\n",
    "    'Correlation': [correlations[f] for f in top_features_for_modeling],\n",
    "    'Abs_Correlation': [abs(correlations[f]) for f in top_features_for_modeling]\n",
    "}).sort_values('Abs_Correlation', ascending=False)\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 80)\n",
    "print(\"Feature Importance Summary:\")\n",
    "print(feature_importance_df.head(20).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cleaned dataset with selected features for modeling\n",
    "# Include: top features + target + identifiers + categorical features\n",
    "features_to_keep = top_features_for_modeling + ['called_up', 'player_name', 'season_year']\n",
    "\n",
    "# Add categorical features if they exist\n",
    "if 'position' in df_clean.columns:\n",
    "    features_to_keep.append('position')\n",
    "\n",
    "# Keep only columns that exist\n",
    "features_to_keep = [f for f in features_to_keep if f in df_clean.columns]\n",
    "\n",
    "df_modeling = df_clean[features_to_keep].copy()\n",
    "\n",
    "print(f\"✅ Cleaned dataset for modeling created\")\n",
    "print(f\"   Shape: {df_modeling.shape}\")\n",
    "print(f\"   Features: {len(features_to_keep) - 3} (excluding target, player_name, season_year)\")\n",
    "print(f\"   Target variable: called_up\")\n",
    "print(f\"First few rows:\")\n",
    "df_modeling.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b244fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset for modeling\n",
    "output_file = 'data/prediction_dataset_cleaned.csv'\n",
    "df_modeling.to_csv(output_file, index=False)\n",
    "print(f\"✅ Cleaned dataset saved to: {output_file}\")\n",
    "\n",
    "# Also save feature importance\n",
    "feature_importance_file = 'data/feature_importance.csv'\n",
    "feature_importance_df.to_csv(feature_importance_file, index=False)\n",
    "print(f\"✅ Feature importance saved to: {feature_importance_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e458d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 9: Key Findings and Recommendations\n",
    "\n",
    "### Summary of Findings\n",
    "\n",
    "1. **Data Filtering**: The 2021 season (pandemic year) was excluded from analysis to ensure patterns reflect normal call-up behavior.\n",
    "\n",
    "2. **Class Imbalance**: The dataset has significant class imbalance (~8.3% call-up rate), requiring special handling in modeling.\n",
    "\n",
    "3. **Top Predictive Features**: Based on correlation analysis, the most important features for predicting call-ups are:\n",
    "   - Performance metrics (points, rebounds, assists, efficiency)\n",
    "   - Shooting percentages (field goal %, three-point %, true shooting %)\n",
    "   - Usage and playing time metrics\n",
    "   - Advanced statistics (PER, efficiency ratings)\n",
    "\n",
    "4. **Data Quality**: \n",
    "   - Missing values were minimal and handled appropriately\n",
    "   - No significant data quality issues found\n",
    "   - Dataset is ready for modeling\n",
    "\n",
    "### Recommendations for Modeling\n",
    "\n",
    "1. **Feature Selection**: Use the top 20-30 features identified in correlation analysis\n",
    "2. **Class Imbalance Handling**: \n",
    "   - Use class weights in models\n",
    "   - Consider SMOTE for oversampling\n",
    "   - Use stratified train/test splits\n",
    "3. **Evaluation Metrics**: Focus on F1-score, precision, and recall rather than accuracy\n",
    "4. **Model Selection**: Try Random Forest, XGBoost, and Logistic Regression with class weights\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Proceed to `analysis.ipynb` for model building\n",
    "2. Use the cleaned dataset: `data/prediction_dataset_cleaned.csv`\n",
    "3. Reference feature importance: `data/feature_importance.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217ff6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary statistics\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL DATASET SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Original Dataset:\")\n",
    "print(f\"  - Shape: {df.shape}\")\n",
    "print(f\"  - Features: {df.shape[1]}\")\n",
    "print(f\"Cleaned Dataset:\")\n",
    "print(f\"  - Shape: {df_modeling.shape}\")\n",
    "print(f\"  - Features: {len(features_to_keep)}\")\n",
    "print(f\"  - Target: called_up\")\n",
    "print(f\"Target Distribution:\")\n",
    "print(f\"  - Called Up: {df_modeling['called_up'].sum():,} ({df_modeling['called_up'].mean()*100:.2f}%)\")\n",
    "print(f\"  - Not Called Up: {(df_modeling['called_up'] == 0).sum():,} ({(df_modeling['called_up'] == 0).mean()*100:.2f}%)\")\n",
    "print(f\"Top 10 Features Selected:\")\n",
    "for idx, feature in enumerate(top_features_for_modeling[:10], 1):\n",
    "    print(f\"  {idx:2d}. {feature} (corr: {correlations[feature]:.4f})\")\n",
    "print(\"\" + \"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
